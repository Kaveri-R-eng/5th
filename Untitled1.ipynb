{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fefeafa4-9a5e-4da4-aaea-b74383866dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rkave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "995a11aa-5050-4a69-b6e3-1dd3ff8c5e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smartphones have become an essential part of our daily lives, offering communication, entertainment, and productivity all in one device. The rapid advancements in smartphone technology have led to features like facial recognition, high-resolution cameras, and 4G connectivity.\n",
      "\n",
      "Number of sentences: 2\n",
      "\n",
      "Sentence: 1 \n",
      " Smartphones have become an essential part of our daily lives, offering communication, entertainment, and productivity all in one device.\n",
      "\n",
      "Sentence: 2 \n",
      " The rapid advancements in smartphone technology have led to features like facial recognition, high-resolution cameras, and 4G connectivity.\n",
      "\n",
      "Total Words: 44\n",
      "['Smartphones', 'have', 'become', 'an', 'essential', 'part', 'of', 'our', 'daily', 'lives', ',', 'offering', 'communication', ',', 'entertainment', ',', 'and', 'productivity', 'all', 'in', 'one', 'device', '.', 'The', 'rapid', 'advancements', 'in', 'smartphone', 'technology', 'have', 'led', 'to', 'features', 'like', 'facial', 'recognition', ',', 'high-resolution', 'cameras', ',', 'and', '4G', 'connectivity', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download the punkt tokenizer mod\n",
    "\n",
    "# Open the file and read the content\n",
    "with open(\"D:\\\\csss\\\\da.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Print the text\n",
    "print(text)\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sent = sent_tokenize(text)\n",
    "print(\"Number of sentences:\", len(sent))\n",
    "for i in range(len(sent)):\n",
    "    print(\"\\nSentence:\", i + 1, \"\\n\", sent[i])\n",
    "\n",
    "# Tokenize the text into words\n",
    "w = word_tokenize(text)\n",
    "print(\"\\nTotal Words:\", len(w))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4a087af-e1e6-47a4-bf31-9db0825c5e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\rkave\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from keras) (0.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from keras) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from optree->keras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rkave\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f96591-c8f8-4d39-8126-5c6b7d4affb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b438e7ce-6f61-4b68-85a2-24b255470162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Load the IMDB dataset\n",
    "# num_words=5000 means we only consider the top 5000 most frequent words\n",
    "max_words = 5000\n",
    "max_len = 500  # We will pad/truncate reviews to 500 words\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "\n",
    "# Pad sequences to ensure each input sequence has the same length\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128, input_length=max_len))  # Embedding layer\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))   # LSTM layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(f'Test accuracy: {acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8a5ea-3f3c-4502-be50-f1cd7e971460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
