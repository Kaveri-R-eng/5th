{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5151334-185a-4ef8-af0b-b254126349e2",
   "metadata": {},
   "source": [
    "# 1.Project Setup # Objective: Predict future Bitcoin prices using historical data and machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84ca49-cf23-4c7d-9d9d-62815ab8c83e",
   "metadata": {},
   "source": [
    "# 2. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee5061-4ca7-4aa0-a625-e1dba055a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"D:\\\\csss\\\\coinmarketcap_06122017.csv\")\n",
    "df.head()\n",
    "print(df.info())\n",
    "#df = pd.read_csv(\"D:\\\\csss\\\\coinmarketcap_06122017.csv\")  # Make sure you're reading a CSV or creating a DataFrame\n",
    "\n",
    "# Display basic statistics for numerical columns\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bef871-fd95-4b98-bf94-99aa719af15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum of null\",df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999bae7b-eac9-4f09-b914-0de6a8bdfa4d",
   "metadata": {},
   "source": [
    "#  Remove the attributes with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33db483-f961-47a9-8a71-91112449c789",
   "metadata": {},
   "source": [
    "# .3 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b46df-00cc-4dd2-90a2-6c53e16fd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"D:\\\\csss\\\\coinmarketcap_06122017.csv\")\n",
    "\n",
    "# Show the number of null values in each column before filling\n",
    "print(\"Sum of null values before filling:\\n\", df.isna().sum())\n",
    "\n",
    "# Replace missing values in 'max_supply' with the mean\n",
    "print(\"Replacing missing values with mean in 'max_supply'\")\n",
    "print(\"Before Replacing max_supply:\\n\", df['max_supply'].head(7))\n",
    "mean_value = df['max_supply'].mean()\n",
    "print(\"\\nMean of 'max_supply' column:\", mean_value)\n",
    "df['max_supply'] = df['max_supply'].fillna(mean_value)\n",
    "print(\"\\nAfter Replacing with Mean:\\n\", df['max_supply'].head(7))\n",
    "\n",
    "# Replace missing values in 'available_supply' with the median\n",
    "print(\"\\nReplacing missing values with median in 'available_supply'\")\n",
    "print(\"Before Replacing available_supply:\\n\", df['available_supply'].head(7))\n",
    "median_value = df['available_supply'].median()\n",
    "print(\"\\nMedian of 'available_supply' column:\", median_value)\n",
    "df['available_supply'] = df['available_supply'].fillna(median_value)\n",
    "print(\"\\nAfter Replacing with Median:\\n\", df['available_supply'].head(7))\n",
    "\n",
    "# Replace missing values in 'market_cap_usd' with the mode\n",
    "print(\"\\nReplacing missing values with mode in 'market_cap_usd'\")\n",
    "print(\"Before Replacing market_cap_usd:\\n\", df['market_cap_usd'].head(7))\n",
    "mode_value = df['market_cap_usd'].mode()[0]\n",
    "print(\"\\nMode of 'market_cap_usd' column:\", mode_value)\n",
    "df['market_cap_usd'] = df['market_cap_usd'].fillna(mode_value)\n",
    "print(\"\\nAfter Replacing with Mode:\\n\", df['market_cap_usd'].head(7))\n",
    "\n",
    "# Replace missing values in 'total_supply' with the mode\n",
    "print(\"\\nReplacing missing values with mode in 'total_supply'\")\n",
    "print(\"Before Replacing total_supply:\\n\", df['total_supply'].head(7))\n",
    "mode_value_total_supply = df['total_supply'].mode()[0]\n",
    "print(\"\\nMode of 'total_supply' column:\", mode_value_total_supply)\n",
    "df['total_supply'] = df['total_supply'].fillna(mode_value_total_supply)\n",
    "print(\"\\nAfter Replacing with Mode:\\n\", df['total_supply'].head(7))\n",
    "\n",
    "# Replace missing values in 'percent_change_7d' with the mode\n",
    "percent_change_7d_mode = df['percent_change_7d'].mode()[0]\n",
    "print(\"\\nMode of 'percent_change_7d':\", percent_change_7d_mode)\n",
    "df['percent_change_7d'] = df['percent_change_7d'].fillna(percent_change_7d_mode)\n",
    "\n",
    "# Replace missing values in 'percent_change_24h' with the mode\n",
    "percent_change_24h_mode = df['percent_change_24h'].mode()[0]\n",
    "print(\"\\nMode of 'percent_change_24h':\", percent_change_24h_mode)\n",
    "df['percent_change_24h'] = df['percent_change_24h'].fillna(percent_change_24h_mode)\n",
    "\n",
    "# Replace remaining NaN values with -1\n",
    "df = df.replace({np.nan: -1})\n",
    "\n",
    "# Show the number of null values in each column after filling\n",
    "print(\"\\nSum of null values after filling:\\n\", df.isna().sum())\n",
    "\n",
    "# Calculate 7-day and 30-day moving averages for 'price_usd'\n",
    "df['7_day_moving_avg'] = df['price_usd'].rolling(window=7).mean()\n",
    "df['30_day_moving_avg'] = df['price_usd'].rolling(window=30).mean()\n",
    "\n",
    "# Calculate price momentum as the difference between the current price and the price 7 days ago\n",
    "df['price_momentum_7d'] = df['price_usd'] - df['price_usd'].shift(7)\n",
    "\n",
    "# Calculate the Relative Strength Index (RSI)\n",
    "def calculate_rsi(series, period=14):\n",
    "    delta = series.diff(1)\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df['RSI_14'] = calculate_rsi(df['price_usd'], 14)\n",
    "\n",
    "# Create 1-day and 7-day lag features for 'price_usd'\n",
    "df['price_usd_lag1'] = df['price_usd'].shift(1)\n",
    "df['price_usd_lag7'] = df['price_usd'].shift(7)\n",
    "\n",
    "# Calculate the difference between 7-day and 30-day moving averages\n",
    "df['moving_avg_diff'] = df['7_day_moving_avg'] - df['30_day_moving_avg']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd711127-f0e3-4193-ad41-2d78f833ee7b",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4dc3c1-e8d4-420b-97af-33153b16e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the original price with the 7-day and 30-day moving averages to smooth out noise and identify underlying trends.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the 'last_updated' Unix timestamp to datetime\n",
    "df['date'] = pd.to_datetime(df['last_updated'], unit='s')\n",
    "# Calculate moving averages\n",
    "df['7_day_moving_avg'] = df['price_usd'].rolling(window=7).mean()\n",
    "df['30_day_moving_avg'] = df['price_usd'].rolling(window=30).mean()\n",
    "# Plottings\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df['date'], df['price_usd'], label='Price USD', color='blue')\n",
    "plt.plot(df['date'], df['7_day_moving_avg'], label='7-Day Moving Average', color='red')\n",
    "plt.plot(df['date'], df['30_day_moving_avg'], label='30-Day Moving Average', color='green')\n",
    "plt.title('Historical Price and Moving Averages')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price USD')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f38088-6809-4ed9-a882-1ac03d675187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get the number of numeric columns\n",
    "num_columns = len(df.select_dtypes(include=np.number).columns)\n",
    "\n",
    "# Determine the number of rows required for the subplots\n",
    "num_rows = (num_columns // 4) + (num_columns % 4 > 0)\n",
    "\n",
    "# Step 1: Show Outliers using Boxplots\n",
    "plt.figure(figsize=(16, num_rows * 4))\n",
    "for i, column in enumerate(df.select_dtypes(include=np.number).columns):\n",
    "    plt.subplot(num_rows, 4, i+1)  # Adjusted to handle different number of columns\n",
    "    sns.boxplot(data=df, x=column)\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Show Distribution and Frequency Distribution using Histograms and KDE\n",
    "plt.figure(figsize=(16, num_rows * 4))\n",
    "for i, column in enumerate(df.select_dtypes(include=np.number).columns):\n",
    "    plt.subplot(num_rows, 4, i+1)\n",
    "    sns.histplot(df[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Show Correlation Matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(\"Correlation Matrix:\\n\", correlation_matrix)\n",
    "\n",
    "# Step 4: Create Scatter Plots for Pairwise Relationships\n",
    "sns.pairplot(df.select_dtypes(include=np.number))\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Create a Heatmap for Correlation Matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Heatmap of Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b33e4e-4589-4e40-915b-54230fb43e2f",
   "metadata": {},
   "source": [
    "# . Preprocessing the Data\n",
    "The first step is to prepare the data for training, ensuring the input features are properly scaled for SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd30df-9255-4f7d-a706-c30f7ab29080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Features to use for regression\n",
    "X = df[['24h_volume_usd', 'available_supply', 'market_cap_usd', 'max_supply', \n",
    "        'percent_change_1h', 'percent_change_24h', 'percent_change_7d', \n",
    "        'price_btc', 'total_supply']]\n",
    "\n",
    "# Target variable (using price_usd as the target)\n",
    "y = df['price_usd']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (SVR requires feature scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e393871-56eb-446d-bc5e-1c138ac41c21",
   "metadata": {},
   "source": [
    "# 2. Support Vector Regression (SVR)\n",
    "Support Vector Regression (SVR) is sensitive to feature scaling, so we scaled the data in the previous step. Now, we train the SVR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fcf56-69c6-4214-8ad9-232c652513a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize and train the SVR model\n",
    "svr_model = SVR(kernel='rbf')  # 'rbf' kernel for non-linear regression\n",
    "svr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions with SVR\n",
    "y_pred_svr = svr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the SVR model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "print(\"SVR Model Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", mse_svr)\n",
    "print(\"R-squared (R2):\", r2_svr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5c052-564e-4eff-ac41-6464d292afc4",
   "metadata": {},
   "source": [
    "# 3. Random Decision Trees (ExtraTreesRegressor)\n",
    "For Random Decision Trees, you can use ExtraTreesRegressor, which is a type of ensemble learning model that creates multiple random decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8fdc0-f42a-448c-9987-109a3f3ffa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# Initialize and train the ExtraTreesRegressor (Random Decision Trees)\n",
    "etr_model = ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
    "etr_model.fit(X_train, y_train)  # No scaling required for ExtraTreesRegressor\n",
    "\n",
    "# Make predictions with ExtraTreesRegressor\n",
    "y_pred_etr = etr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the ExtraTreesRegressor model\n",
    "mse_etr = mean_squared_error(y_test, y_pred_etr)\n",
    "r2_etr = r2_score(y_test, y_pred_etr)\n",
    "\n",
    "print(\"Random Decision Trees (ExtraTreesRegressor) Performance:\")\n",
    "print(\"Mean Squared Error (MSE):\", mse_etr)\n",
    "print(\"R-squared (R2):\", r2_etr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448bf9d-3f62-4ce3-8387-5deaf899a627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
